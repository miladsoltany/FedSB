# Federated Domain Generalization with Label Smoothing and Balanced Decentralized Training (FedSB)
![Overview v3](https://github.com/user-attachments/assets/c42f211d-024f-409b-8a17-89f6f53baf47)

This repository contains the implementation of **FedSB**, a novel approach to federated domain generalization that addresses the challenges of data heterogeneity through:
1. **Label Smoothing**: Mitigates overconfidence of local models, enhancing domain generalization.
2. **Balanced Decentralized Training**: Introduces a budgeting mechanism to ensure consistent contributions from all clients.

### Abstract
FedSB utilizes label smoothing at the client level to prevent overfitting to domain-specific features and employs a decentralized budgeting mechanism to balance training contributions. Our experiments on four multi-domain datasets (PACS, VLCS, OfficeHome, and TerraIncognita) demonstrate state-of-the-art performance on three of the four datasets.

### Key Features
- Enhanced domain generalization through label smoothing.
- Balanced contributions from clients with varying data sizes.
- Outperforms competing methods on major benchmarks.

### Datasets Used
- **PACS**
- **OfficeHome**
- **VLCS**
- **TerraIncognita**

### Results
FedSB achieves state-of-the-art results on three out of four datasets, significantly advancing federated learning and domain generalization.

### Paper
For more details, refer to our [paper](https://arxiv.org/abs/2412.11408).

### Code
The code for this repository is **coming soon**. Stay tuned for updates!
